{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Machine Learning Primer - Workshop\n",
    "# Day 1 - September 2021\n",
    "####################################################################\n",
    "\n",
    "# python package imports\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be53c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Functions to generate (fake) data that we would use for supervised\n",
    "# linear regression\n",
    "####################################################################\n",
    "\n",
    "def line_hypothesis(x: np.ndarray, m: float, c: float) -> np.array:\n",
    "    return (m * x) + c\n",
    "\n",
    "def generate_fake_single_var_data(\n",
    "    n_points: int,\n",
    "    true_m: float,\n",
    "    true_c: float,\n",
    "    min_x: float,\n",
    "    max_x: float,\n",
    "    noise_std: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate fake noisy data from a hypothetical line\n",
    "    \n",
    "    Args:\n",
    "        n_points: number of sample data points (x, y) to generate.\n",
    "        true_m: the gradient to use for the hypothetical line.\n",
    "        true_c: the intercept to use for the hypothetical line.\n",
    "        min_x: minimum value that x can take.\n",
    "        max_x: maximum value that x can take.\n",
    "        noise_std: noise standard-deviation to add to generate y for\n",
    "            each data point.\n",
    "    \n",
    "    Returns:\n",
    "        x: Vector of size (n_points), giving each sample's x value.\n",
    "        noisy_y: Vector of size (n_points), giving each sample's y value.\n",
    "    \"\"\"\n",
    "    # generate the underlying x points\n",
    "    x = np.random.uniform(low=min_x, high=max_x, size=n_points)\n",
    "    # apply the line equation to get the ground-truth y for each x\n",
    "    y = line_hypothesis(x, m=true_m, c=true_c)\n",
    "    # add noise to y\n",
    "    noise = np.random.normal(scale=noise_std, size=n_points)\n",
    "    noisy_y = y + noise\n",
    "    return x, noisy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Data plotting functionality\n",
    "####################################################################\n",
    "\n",
    "def get_hypothesis_scatter_plots(\n",
    "    data_x: np.ndarray,\n",
    "    data_y: np.ndarray,\n",
    "    hypothesis_m: Optional[float] = None,\n",
    "    hypothesis_c: Optional[float] = None,\n",
    ") -> List[go.Scatter]:\n",
    "    \"\"\"Gets the scatter plots to draw the data, true line, and hypothesis\n",
    "    line if parameters given.\n",
    "    \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        hypothesis_m: The m (slope) hypothesis. If given, will be used\n",
    "            to draw the hypothesis line.\n",
    "        hypothesis_c: The c (intercept) hypothesis. If given, will be used\n",
    "            to draw the hypothesis line.\n",
    "    \n",
    "    Returns:\n",
    "        List of scatter plots for drawing data, true line, and hypothesis.\n",
    "    \"\"\"\n",
    "    scatter_plots = [\n",
    "        # plot the data\n",
    "        go.Scatter(\n",
    "            x=data_x,\n",
    "            y=data_y,\n",
    "            mode=\"markers\",\n",
    "            marker_size=10,\n",
    "            marker_color='blue',\n",
    "            name=\"data\"\n",
    "        ),\n",
    "        # plot the underlying GT line\n",
    "        go.Scatter(\n",
    "            x=[min_x, max_x],\n",
    "            y=line_hypothesis(np.array([min_x, max_x]), m=true_m, c=true_c),\n",
    "            mode='lines',\n",
    "            line_dash='dash',\n",
    "            line_color='green',\n",
    "            name='ground-truth'\n",
    "        )\n",
    "    ]\n",
    "    if hypothesis_m is not None:\n",
    "        # plot the current hypothesis\n",
    "        scatter_plots.append(\n",
    "            go.Scatter(\n",
    "                x=[min_x, max_x],\n",
    "                y=line_hypothesis(np.array([min_x, max_x]), m=hypothesis_m, c=hypothesis_c),\n",
    "                mode='lines',\n",
    "                line_dash='dash',\n",
    "                line_color='red',\n",
    "                name='hypothesis'\n",
    "            )\n",
    "        )\n",
    "    return scatter_plots\n",
    "\n",
    "\n",
    "def plot_gradient_descent_info(\n",
    "    data_x: np.ndarray,\n",
    "    data_y: np.ndarray,\n",
    "    m_history: np.ndarray,\n",
    "    c_history: np.ndarray,\n",
    "):\n",
    "    \"\"\"Draws data/hypothesis; cost contour map; and cost vs training iterations.\n",
    "        \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        m_history: Vector of m (slope) values as training progresses\n",
    "            (from the oldest to the newest).\n",
    "        c_history: Vector of c (intercept) values as training progresses\n",
    "            (from the oldest to the newest).\n",
    "    \"\"\"\n",
    "    common_axis = dict(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linewidth=2,\n",
    "        linecolor='black'\n",
    "    )\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[[{}, {}], [{\"colspan\": 2}, None]],\n",
    "        row_heights=[0.7, 0.3],\n",
    "        # horizontal_spacing=0.01,\n",
    "        vertical_spacing=0.2,\n",
    "        subplot_titles=(\"Hypothesis\", \"Cost map\", \"Cost w/ iteration\")\n",
    "    )\n",
    "    \n",
    "    # plot the data, the hypothesis, and true line\n",
    "    scatter_plots = get_hypothesis_scatter_plots(\n",
    "        data_x=data_x,\n",
    "        data_y=data_y,\n",
    "        hypothesis_m=m_history[-1],\n",
    "        hypothesis_c=c_history[-1],\n",
    "    )\n",
    "    for plot in scatter_plots:\n",
    "        fig.add_trace(plot, row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"time (mins)\", row=1, col=1, **common_axis)\n",
    "    fig.update_yaxes(title_text=\"distance (km)\", row=1, col=1, **common_axis)\n",
    "    \n",
    "    # plot the cost contour map against m and c parameters\n",
    "    fig.add_trace(\n",
    "        go.Contour(x=m_plot_range, y=c_plot_range, z=grid_costs),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    # plot the history of m/c values on the cost contour map\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=m_history,\n",
    "            y=c_history,\n",
    "            mode=\"markers+lines\",\n",
    "            marker_size=[5]*(len(m_history)-1) + [10],\n",
    "            marker_color='white',\n",
    "            name=\"m/c history\"\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    # draw a marker for true m/c \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[true_m],\n",
    "            y=[true_c],\n",
    "            mode=\"markers\",\n",
    "            marker_size=10,\n",
    "            marker_color='yellow',\n",
    "            marker_symbol='x',\n",
    "            name=\"True m/c\"\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"m (gradient)\",\n",
    "        range=[np.min(m_plot_range), np.max(m_plot_range)],\n",
    "        row=1, col=2, **common_axis\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"c (intercept)\",\n",
    "        range=[np.min(c_plot_range), np.max(c_plot_range)],\n",
    "        row=1, col=2, **common_axis\n",
    "    )\n",
    "    \n",
    "    # plot history of the cost against training iterations\n",
    "    cost_history = [\n",
    "        compute_cost(data_x=data_x, data_y=data_y, curr_m=curr_m, curr_c=curr_c)\n",
    "        for curr_m, curr_c in zip(m_history, c_history)\n",
    "    ]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(1, len(cost_history) + 1)),\n",
    "            y=cost_history,\n",
    "            mode=\"markers+lines\",\n",
    "            name=\"cost\",\n",
    "        ),\n",
    "        row=2, col=1,\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Step\", row=2, col=1, **common_axis)\n",
    "    fig.update_yaxes(title_text=\"Cost\", row=2, col=1, **common_axis)\n",
    "    \n",
    "    # move the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.05,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Generate some (fake) data to do supervised regression\n",
    "####################################################################\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "N = 20\n",
    "min_x = 0\n",
    "max_x = 1\n",
    "\n",
    "true_m = 0.5  # np.random.uniform(low=0, high=1)\n",
    "true_c = 0.5  # np.random.uniform(low=0, high=1)\n",
    "\n",
    "# generate the data from a fake randomly generated line\n",
    "x, y = generate_fake_single_var_data(\n",
    "    n_points=N,\n",
    "    true_m=true_m,\n",
    "    true_c=true_c,\n",
    "    min_x=min_x,\n",
    "    max_x=max_x,\n",
    "    noise_std=0.05,\n",
    ")\n",
    "\n",
    "####################################################################\n",
    "# Plot the generated data (and the underlying randomly generated line)\n",
    "####################################################################\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=dict(\n",
    "        xaxis_title=\"time (mins)\",\n",
    "        yaxis_title=\"distance (km)\",\n",
    "        title=f\"Line data: m: {true_m}, c: {true_c}\",\n",
    "        title_x=0.5,\n",
    "    )\n",
    ")\n",
    "\n",
    "scatter_plots = get_hypothesis_scatter_plots(data_x=x, data_y=y)\n",
    "for plot in scatter_plots:\n",
    "    fig.add_trace(plot)\n",
    "\n",
    "fig.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(\n",
    "    data_x: np.ndarray,\n",
    "    data_y: np.ndarray,\n",
    "    curr_m: float,\n",
    "    curr_c: float\n",
    ") -> float:\n",
    "    \"\"\"Compute the cost at certain value of m (slope) and c (intercept).\n",
    "    \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        curr_m: The m (slope) value to compute the cost at\n",
    "        curr_c: The c (intercept) value to compute the cost at\n",
    "    \n",
    "    Returns:\n",
    "        Cost scalar value\n",
    "    \"\"\"\n",
    "    hypothesis_y = line_hypothesis(data_x, m=curr_m, c=curr_c)\n",
    "    cost = (hypothesis_y - data_y) ** 2\n",
    "    cost = np.mean(cost, axis=-1) / 2\n",
    "    return cost\n",
    "\n",
    "\n",
    "def gradient_m(\n",
    "    data_x: np.ndarray,\n",
    "    data_y: np.ndarray,\n",
    "    curr_m: float,\n",
    "    curr_c: float\n",
    ") -> float:\n",
    "    \"\"\"Compute the gradient of the cost with respect to m (slope)\n",
    "    \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        curr_m: The m (slope) value to compute the gradient at\n",
    "        curr_c: The c (intercept) value to compute the gradient at\n",
    "    \n",
    "    Returns:\n",
    "        Gradient of cost with respect to m (slope)\n",
    "    \"\"\"\n",
    "    grad_m = (curr_c + (curr_m * data_x) - data_y) * data_x\n",
    "    return np.sum(grad_m) / y.size\n",
    "\n",
    "\n",
    "def gradient_c(\n",
    "    data_x: np.ndarray,\n",
    "    data_y: np.ndarray,\n",
    "    curr_m: float,\n",
    "    curr_c: float\n",
    ") -> float:\n",
    "    \"\"\"Compute the gradient of the cost with respect to c (intercept)\n",
    "    \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        curr_m: The m (slope) value to compute the gradient at\n",
    "        curr_c: The c (intercept) value to compute the gradient at\n",
    "    \n",
    "    Returns:\n",
    "        Gradient of cost with respect to c (intercept)\n",
    "    \"\"\"\n",
    "    grad_c = curr_c + (curr_m * data_x) - data_y\n",
    "    return np.sum(grad_c) / data_y.size\n",
    "\n",
    "\n",
    "    \n",
    "def gradient_descent_step(\n",
    "    data_x: np.ndarray,\n",
    "    data_y: np.ndarray,\n",
    "    curr_m: float,\n",
    "    curr_c: float,\n",
    "    learning_rate: float\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Runs a single step of gradient descent\n",
    "    \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        curr_m: The m (slope) value to do gradient descent at\n",
    "        curr_c: The c (intercept) value to do gradient descent at\n",
    "        learning_rate: Step size for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "        new_m: The new scalar value for m (slope) after a single step of\n",
    "            gradient descent.\n",
    "        new_c: The new scalar value for c (intercept) after a single step of\n",
    "            gradient descent.\n",
    "    \"\"\"\n",
    "    # compute the gradients\n",
    "    grad_m = gradient_m(data_x=data_x, data_y=data_y, curr_m=curr_m, curr_c=curr_c)\n",
    "    grad_c = gradient_c(data_x=data_x, data_y=data_y, curr_m=curr_m, curr_c=curr_c)\n",
    "    \n",
    "    new_m = curr_m - learning_rate * grad_m\n",
    "    new_c = curr_c - learning_rate * grad_c\n",
    "    \n",
    "    return new_m, new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Get costs at different m/c parameters so we can draw a contour\n",
    "# plot showing how the cost changes with different parameter values\n",
    "####################################################################\n",
    "\n",
    "# the range of m/c values\n",
    "m_plot_range = np.linspace(0, 1, 101)\n",
    "c_plot_range = np.linspace(0, 1, 101)\n",
    "\n",
    "grid_m, grid_c = np.meshgrid(m_plot_range, c_plot_range)\n",
    "grid_m = np.reshape(grid_m, (-1))\n",
    "grid_c = np.reshape(grid_c, (-1))\n",
    "\n",
    "# compute the cost at each parameter value\n",
    "grid_costs = compute_cost(\n",
    "    data_x=x, data_y=y, curr_m=grid_m[:, np.newaxis], curr_c=grid_c[:, np.newaxis]\n",
    ")\n",
    "grid_costs = np.reshape(grid_costs, (c_plot_range.size, m_plot_range.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e8675",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Run gradient descent for n_steps\n",
    "####################################################################\n",
    "\n",
    "n_steps = 1\n",
    "\n",
    "# some initial hypothesis (can be any random value)\n",
    "hypothesis_m = 0.1\n",
    "hypothesis_c = 0.9\n",
    "\n",
    "# the step size used during gradient descent\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# used for plotting the progress of the parameters\n",
    "m_history = [hypothesis_m]\n",
    "c_history = [hypothesis_c]\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    # compute the current cost (loss)\n",
    "    cost = compute_cost(\n",
    "        data_x=x,\n",
    "        data_y=y,\n",
    "        curr_m=hypothesis_m,\n",
    "        curr_c=hypothesis_c\n",
    "    )\n",
    "    \n",
    "    # get the new slope / intercept by gradient descent\n",
    "    hypothesis_m, hypothesis_c = gradient_descent_step(\n",
    "        data_x=x,\n",
    "        data_y=y,\n",
    "        curr_m=hypothesis_m,\n",
    "        curr_c=hypothesis_c,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # print(f\"Step {idx: <3},  Cost: {cost:.5f},  {hypothesis_m},  {hypothesis_c}\")\n",
    "\n",
    "    m_history.append(hypothesis_m)\n",
    "    c_history.append(hypothesis_c)\n",
    "\n",
    "# plot the whole gradient descent process\n",
    "plot_gradient_descent_info(\n",
    "    data_x=x,\n",
    "    data_y=y,\n",
    "    m_history=np.array(m_history),\n",
    "    c_history=np.array(c_history)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fd24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
