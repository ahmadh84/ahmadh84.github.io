{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f510ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Machine Learning Primer - Workshop\n",
    "# Day 3 - September 2021\n",
    "####################################################################\n",
    "\n",
    "# python package imports\n",
    "from __future__ import annotations\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Functions to generate (fake) data that we would use for neural\n",
    "# network regression\n",
    "####################################################################\n",
    "\n",
    "def gt_function(x: float | np.ndarray, true_params: np.ndarray):\n",
    "    return (true_params[0] * x) + (true_params[1] * x) + (true_params[2])\n",
    "\n",
    "def generate_fake_single_var_data(\n",
    "    n_points: int,\n",
    "    min_x: float,\n",
    "    max_x: float,\n",
    "    true_params: np.ndarray,\n",
    "    noise_std: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate fake noisy data from a hypothetical line\n",
    "    \n",
    "    Args:\n",
    "        n_points: number of sample data points (x, y) to generate.\n",
    "        true_m: the gradient to use for the hypothetical line.\n",
    "        true_c: the intercept to use for the hypothetical line.\n",
    "        noise_std: noise standard-deviation to add to generate y for\n",
    "            each data point.\n",
    "    \n",
    "    Returns:\n",
    "        x: Vector of size (n_points), giving each sample's x value.\n",
    "        noisy_y: Vector of size (n_points), giving each sample's y value.\n",
    "    \"\"\"\n",
    "    # generate the underlying x points\n",
    "    x = np.random.uniform(low=min_x, high=max_x, size=n_points)\n",
    "    # apply the line equation to get the ground-truth y for each x\n",
    "    y = gt_function(x, true_params)\n",
    "    # add noise to y\n",
    "    noise = np.random.normal(scale=noise_std, size=n_points)\n",
    "    noisy_y = y + noise\n",
    "    return x, noisy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Data plotting functionality\n",
    "####################################################################\n",
    "\n",
    "def get_hypothesis_scatter_plots(\n",
    "    data_x: np.ndarray,\n",
    "    data_y: np.ndarray,\n",
    "    predictor: Optional[Callable] = None,\n",
    ") -> List[go.Scatter]:\n",
    "    \"\"\"Gets the scatter plots to draw the data, true line, and hypothesis\n",
    "    line if parameters given.\n",
    "    \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        hypothesis_m: The m (slope) hypothesis. If given, will be used\n",
    "            to draw the hypothesis line.\n",
    "        hypothesis_c: The c (intercept) hypothesis. If given, will be used\n",
    "            to draw the hypothesis line.\n",
    "    \n",
    "    Returns:\n",
    "        List of scatter plots for drawing data, true line, and hypothesis.\n",
    "    \"\"\"\n",
    "    x_range = np.linspace(min_x, max_x, 100)\n",
    "    scatter_plots = [\n",
    "        # plot the data\n",
    "        go.Scatter(\n",
    "            x=data_x,\n",
    "            y=data_y,\n",
    "            mode=\"markers\",\n",
    "            marker_size=10,\n",
    "            marker_color='blue',\n",
    "            name=\"data\"\n",
    "        ),\n",
    "        # plot the underlying GT line\n",
    "        go.Scatter(\n",
    "            x=x_range,\n",
    "            y=gt_function(x_range, true_params),\n",
    "            mode='lines',\n",
    "            line_dash='dash',\n",
    "            line_color='green',\n",
    "            name='ground-truth'\n",
    "        )\n",
    "    ]\n",
    "    if predictor is not None:\n",
    "        # plot the current hypothesis\n",
    "        scatter_plots.append(\n",
    "            go.Scatter(\n",
    "                x=x_range,\n",
    "                y=predictor(x_range),\n",
    "                mode='markers',\n",
    "                marker_size=10,\n",
    "                line_color='red',\n",
    "                name='predictions'\n",
    "            )\n",
    "        )\n",
    "    return scatter_plots\n",
    "\n",
    "\n",
    "def plot_gradient_descent_info(\n",
    "    data_x: np.ndarray,\n",
    "    data_y: np.ndarray,\n",
    "    predictor: Callable,\n",
    "    loss_history: np.array,\n",
    "):\n",
    "    \"\"\"Draws data/hypothesis; cost contour map; and cost vs training iterations.\n",
    "        \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        m_history: Vector of m (slope) values as training progresses\n",
    "            (from the oldest to the newest).\n",
    "        c_history: Vector of c (intercept) values as training progresses\n",
    "            (from the oldest to the newest).\n",
    "    \"\"\"\n",
    "    common_axis = dict(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linewidth=2,\n",
    "        linecolor='black'\n",
    "    )\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=2,\n",
    "        # horizontal_spacing=0.01,\n",
    "        subplot_titles=(\"Test data fit\", \"Loss w/ iterations\")\n",
    "    )\n",
    "    \n",
    "    # plot the data, the hypothesis, and true line\n",
    "    scatter_plots = get_hypothesis_scatter_plots(\n",
    "        data_x=data_x,\n",
    "        data_y=data_y,\n",
    "        predictor=predictor,\n",
    "    )\n",
    "    for plot in scatter_plots:\n",
    "        fig.add_trace(plot, row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"x (input)\", row=1, col=1, **common_axis)\n",
    "    fig.update_yaxes(title_text=\"y (output)\", row=1, col=1, **common_axis)\n",
    "    \n",
    "    # plot history of the cost against training iterations\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(1, loss_history.size + 1)),\n",
    "            y=loss_history,\n",
    "            mode=\"markers+lines\",\n",
    "            name=\"loss\",\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Step\", row=1, col=2, **common_axis)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=2, **common_axis)\n",
    "    \n",
    "    # move the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.05,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Generate some (fake) data to do supervised regression\n",
    "####################################################################\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "N_train = 200\n",
    "N_test = 30\n",
    "\n",
    "min_x = -20\n",
    "max_x = 10\n",
    "noise_std = 0\n",
    "\n",
    "true_params = [10, 20, 1]\n",
    "\n",
    "# generate the data from some underlying non-linear function\n",
    "train_x, train_y = generate_fake_single_var_data(\n",
    "    n_points=N_train,\n",
    "    min_x=min_x,\n",
    "    max_x=max_x,\n",
    "    true_params=true_params,\n",
    "    noise_std=noise_std,\n",
    ")\n",
    "\n",
    "# generate some test data\n",
    "test_x, test_y = generate_fake_single_var_data(\n",
    "    n_points=N_test,\n",
    "    min_x=min_x,\n",
    "    max_x=max_x,\n",
    "    true_params=true_params,\n",
    "    noise_std=noise_std,\n",
    ")\n",
    "\n",
    "####################################################################\n",
    "# Plot the generated data (and the underlying randomly generated line)\n",
    "####################################################################\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=dict(\n",
    "        xaxis_title=\"x (input)\",\n",
    "        yaxis_title=\"y (output)\",\n",
    "        title=f\"Data and the underlying function\",\n",
    "        title_x=0.5,\n",
    "    )\n",
    ")\n",
    "\n",
    "scatter_plots = get_hypothesis_scatter_plots(data_x=train_x, data_y=train_y)\n",
    "for plot in scatter_plots:\n",
    "    fig.add_trace(plot)\n",
    "\n",
    "fig.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data_x: np.ndarray, curr_params: np.ndarray) -> np.ndarray:\n",
    "    w1, w2, w3, w4 = curr_params\n",
    "    # u = w1 + w2.x\n",
    "    u = w1 + w2 * data_x\n",
    "    # v = sigmoid(u)\n",
    "    v = 1 / (1 + np.exp(-u))\n",
    "    # y = w3 + w4.v\n",
    "    y = w3 + w4 * v\n",
    "    return y\n",
    "\n",
    "\n",
    "def l2_loss(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    loss_per_sample = (y_pred - y_true) ** 2\n",
    "    return np.mean(loss_per_sample)\n",
    "\n",
    "\n",
    "def params_grad(\n",
    "    data_x: np.ndarray,\n",
    "    y_true: np.ndarray,\n",
    "    curr_params: np.ndarray\n",
    "):\n",
    "    # forward pass\n",
    "    w1, w2, w3, w4 = curr_params\n",
    "    # u = w1 + w2.x\n",
    "    u = w1 + w2 * data_x\n",
    "    # v = sigmoid(u)\n",
    "    v = 1 / (1 + np.exp(-u))\n",
    "    # y = w3 + w4.v\n",
    "    y = w3 + w4 * v\n",
    "    \n",
    "    grad_per_loss = 2 * (y - y_true)\n",
    "    \n",
    "    grad_w1 = grad_per_loss * w4 * v * (1 - v)\n",
    "    grad_w2 = grad_per_loss * w4 * v * (1 - v) * data_x\n",
    "    grad_w3 = grad_per_loss\n",
    "    grad_w4 = grad_per_loss * v\n",
    "    \n",
    "    grad_w1 = np.mean(grad_w1)\n",
    "    grad_w2 = np.mean(grad_w2)\n",
    "    grad_w3 = np.mean(grad_w3)\n",
    "    grad_w4 = np.mean(grad_w4)\n",
    "    \n",
    "    grad_w = np.array([grad_w1, grad_w2, grad_w3, grad_w4])\n",
    "    return grad_w\n",
    "\n",
    "    \n",
    "def gradient_descent_step(\n",
    "    data_x: np.ndarray,\n",
    "    y_true: np.ndarray,\n",
    "    curr_params: np.ndarray,\n",
    "    learning_rate: float\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Runs a single step of gradient descent\n",
    "    \n",
    "    Args:\n",
    "        data_x: Vector of x values\n",
    "        data_y: Vector of corresponding y values\n",
    "        curr_m: The m (slope) value to do gradient descent at\n",
    "        curr_c: The c (intercept) value to do gradient descent at\n",
    "        learning_rate: Step size for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "        new_m: The new scalar value for m (slope) after a single step of\n",
    "            gradient descent.\n",
    "        new_c: The new scalar value for c (intercept) after a single step of\n",
    "            gradient descent.\n",
    "    \"\"\"\n",
    "    # compute the gradients\n",
    "    grad_w = params_grad(\n",
    "        data_x=data_x,\n",
    "        y_true=y_true,\n",
    "        curr_params=curr_params,\n",
    "    )\n",
    "    \n",
    "    new_params = curr_params - learning_rate * grad_w\n",
    "    \n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38119a31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Run gradient descent for n_steps\n",
    "####################################################################\n",
    "\n",
    "n_steps = 100\n",
    "\n",
    "# some initial weights for the network\n",
    "curr_params = np.array([1, 1, 1, 1])\n",
    "\n",
    "# the step size used during gradient descent\n",
    "learning_rate = 1e-1\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    # forward pass through the network\n",
    "    y_pred = forward_pass(data_x=train_x, curr_params=curr_params)\n",
    "    \n",
    "    # computer the loss\n",
    "    loss = l2_loss(y_pred=y_pred, y_true=train_y)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # gradient descent by back-propagation\n",
    "    curr_params = gradient_descent_step(\n",
    "        data_x=train_x,\n",
    "        y_true=train_y,\n",
    "        curr_params=curr_params,\n",
    "        learning_rate=learning_rate,\n",
    "    )\n",
    "    \n",
    "    print(f\"Step {idx: <3},  Cost: {loss:.5f},  {curr_params}\")\n",
    "\n",
    "# # plot the whole gradient descent process\n",
    "plot_gradient_descent_info(\n",
    "    data_x=test_x,\n",
    "    data_y=test_y,\n",
    "    predictor=partial(forward_pass, curr_params=curr_params),\n",
    "    loss_history=np.array(loss_history),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b118bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
