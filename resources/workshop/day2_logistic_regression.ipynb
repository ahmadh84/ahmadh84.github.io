{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Machine Learning Primer - Workshop\n",
    "# Day 2 - September 2021\n",
    "####################################################################\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# python package imports\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be53c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Functions to generate (fake) data that we would use for supervised\n",
    "# logistic regression (classification)\n",
    "####################################################################\n",
    "\n",
    "def line_hypothesis(x: np.ndarray, m: float, c: float) -> np.array:\n",
    "    return (m * x) + c\n",
    "\n",
    "def generate_fake_single_var_data(\n",
    "    n_points: int,\n",
    "    decision_boundary_true_m: float,\n",
    "    decision_boundary_true_c: float,\n",
    "    points_spread: float = 5,\n",
    "    margin: float = 0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate fake positive negative points on both sides of\n",
    "    the hypothetical decision boundary.\n",
    "    \n",
    "    Args:\n",
    "        n_points: number of sample data points (x, y) to generate.\n",
    "            Half of the would be positive, half would be negative.\n",
    "        decision_boundary_true_m: the gradient to use for the\n",
    "            hypothetical decision boundary.\n",
    "        decision_boundary_true_c: the intercept to use for the\n",
    "            hypothetical decision boundary.\n",
    "        points_spread: The large this number the more the points\n",
    "            are spread out.\n",
    "        margin: If positive, points would be pushed away from the\n",
    "            decision boundary by this amount. If negative, there\n",
    "            would intermingling of negative / positive points\n",
    "            (hence making the learning problem harder).\n",
    "    \n",
    "    Returns:\n",
    "        pstv_pnts: Matrix of size (n_points // 2, 2). The first column\n",
    "            are x values of positive points, and second column are y\n",
    "            values of positive points.\n",
    "        ngtv_pnts: Matrix of size (n_points // 2, 2). The first column\n",
    "            are x values of negative points, and second column are y\n",
    "            values of negative points.\n",
    "    \"\"\"\n",
    "    n_pstv_points = n_points // 2\n",
    "    n_ngtv_points = n_points - n_pstv_points\n",
    "\n",
    "    m, c = decision_boundary_true_m, decision_boundary_true_c\n",
    "    line_center_x = -(m * c) / (1 + m**2)\n",
    "    line_center_y = c / (1 + m**2)\n",
    "    \n",
    "    # get the unit length path orthogonal to the decision boundary\n",
    "    unit_ortho_dir_x = -m / ((1 + m**2) ** 0.5)\n",
    "    unit_ortho_dir_y = 1 / ((1 + m**2) ** 0.5)\n",
    "    \n",
    "    # get the unit length path parallel to the decision boundary\n",
    "    unit_prll_dir_x = unit_ortho_dir_y\n",
    "    unit_prll_dir_y = -unit_ortho_dir_x\n",
    "    \n",
    "    # generate positive points\n",
    "    # first randomly sample where each point would mobe parallel and then orthogonal\n",
    "    # to the decision boundary, relative to the center point (line_center_x, line_center_y)\n",
    "    alpha_prll = np.random.uniform(low=-points_spread, high=points_spread, size=n_pstv_points)\n",
    "    alpha_ortho = np.random.uniform(low=margin, high=margin + points_spread, size=n_pstv_points)\n",
    "    \n",
    "    pstv_pnts = [\n",
    "        line_center_x + (alpha_prll * unit_prll_dir_x) + (alpha_ortho * unit_ortho_dir_x),\n",
    "        line_center_y + (alpha_prll * unit_prll_dir_y) + (alpha_ortho * unit_ortho_dir_y),\n",
    "    ]\n",
    "    \n",
    "    # generate negative points (move othogonal to the decision boundary in the opposite dir.)\n",
    "    # first randomly sample where each point would mobe parallel and then orthogonal\n",
    "    # to the decision boundary, relative to the center point (line_center_x, line_center_y)\n",
    "    alpha_prll = np.random.uniform(low=-points_spread, high=points_spread, size=n_ngtv_points)\n",
    "    alpha_ortho = np.random.uniform(low=-(margin + points_spread), high=-margin, size=n_ngtv_points)\n",
    "    \n",
    "    ngtv_pnts = [\n",
    "        line_center_x + (alpha_prll * unit_prll_dir_x) + (alpha_ortho * unit_ortho_dir_x),\n",
    "        line_center_y + (alpha_prll * unit_prll_dir_y) + (alpha_ortho * unit_ortho_dir_y),\n",
    "    ]\n",
    "\n",
    "    pstv_pnts = np.stack(pstv_pnts, axis=-1)\n",
    "    ngtv_pnts = np.stack(ngtv_pnts, axis=-1)\n",
    "\n",
    "    return pstv_pnts, ngtv_pnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Data plotting functionality\n",
    "####################################################################\n",
    "\n",
    "def get_hypothesis_scatter_plots(\n",
    "    pstv_pnts: np.ndarray,\n",
    "    ngtv_pnts: np.ndarray,\n",
    "    hypothesis_params: Optional[np.ndarray] = None,\n",
    ") -> List[go.Scatter]:\n",
    "    \"\"\"Gets the scatter plots to draw the data, true line, and hypothesis\n",
    "    line if parameters given.\n",
    "    \n",
    "    Args:\n",
    "        pstv_data: (N, C) array of positive data points.\n",
    "        pstv_data: (M, C) array of negative data points.\n",
    "        hypothesis_params: (C) vector of current parameters. If given, will\n",
    "            be used to draw the hypothesis line.\n",
    "    \n",
    "    Returns:\n",
    "        List of scatter plots for drawing data, true line, and hypothesis.\n",
    "    \"\"\"    \n",
    "    min_x = min(np.min(pstv_pnts[:, 0]), np.min(ngtv_pnts[:, 0])) - 1\n",
    "    max_x = max(np.max(pstv_pnts[:, 0]), np.max(ngtv_pnts[:, 0])) + 1\n",
    "    \n",
    "    scatter_plots = [\n",
    "        # plot the positive data points\n",
    "        go.Scatter(\n",
    "            x=pstv_pnts[:, 0],\n",
    "            y=pstv_pnts[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker_size=10,\n",
    "            marker_symbol='circle',\n",
    "            marker_color='blue',\n",
    "            name=\"Positive\",\n",
    "        ),\n",
    "        # plot the negative data points\n",
    "        go.Scatter(\n",
    "            x=ngtv_pnts[:, 0],\n",
    "            y=ngtv_pnts[:, 1],\n",
    "            mode=\"markers\",\n",
    "            marker_size=10,\n",
    "            marker_symbol='x',\n",
    "            marker_color='red',\n",
    "            name=\"Negative\",\n",
    "        ),\n",
    "        # plot the underlying GT line\n",
    "        go.Scatter(\n",
    "            x=[min_x, max_x],\n",
    "            y=line_hypothesis(\n",
    "                np.array([min_x, max_x]),\n",
    "                m=decision_boundary_true_m,\n",
    "                c=decision_boundary_true_c\n",
    "            ),\n",
    "            mode='lines',\n",
    "            line_dash='dash',\n",
    "            line_color='green',\n",
    "            name='GT decision bndry'\n",
    "        )\n",
    "    ]\n",
    "    if hypothesis_params is not None:\n",
    "        # a + bx + cy = 0  ->  y = -b/c x - a/c\n",
    "        hypothesis_m = -hypothesis_params[1] / hypothesis_params[2]\n",
    "        hypothesis_c = -hypothesis_params[0] / hypothesis_params[2]\n",
    "        # plot the current hypothesis\n",
    "        scatter_plots.append(\n",
    "            go.Scatter(\n",
    "                x=[min_x, max_x],\n",
    "                y=line_hypothesis(\n",
    "                    np.array([min_x, max_x]),\n",
    "                    m=hypothesis_m,\n",
    "                    c=hypothesis_c\n",
    "                ),\n",
    "                mode='lines',\n",
    "                # line_dash='dash',\n",
    "                line_color='purple',\n",
    "                name='hypothesis'\n",
    "            )\n",
    "        )\n",
    "    return scatter_plots\n",
    "\n",
    "\n",
    "def plot_gradient_descent_info(\n",
    "    pstv_data: np.ndarray,\n",
    "    ngtv_data: np.ndarray,\n",
    "    params_history: np.ndarray,\n",
    "):\n",
    "    \"\"\"Draws data/hypothesis; cost contour map; and cost vs training iterations.\n",
    "        \n",
    "    Args:\n",
    "        pstv_data: (N, C) array of positive data points.\n",
    "        pstv_data: (M, C) array of negative data points.\n",
    "        params_history: (T, C) array giving all the parameters as the training\n",
    "            progresses (from the oldest to the newest).\n",
    "    \"\"\"\n",
    "    common_axis = dict(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linewidth=2,\n",
    "        linecolor='black'\n",
    "    )\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[[{}, {}], [{\"colspan\": 2}, None]],\n",
    "        row_heights=[0.7, 0.3],\n",
    "        # horizontal_spacing=0.01,\n",
    "        vertical_spacing=0.2,\n",
    "        subplot_titles=(\"Hypothesis\", \"Cost map\", \"Cost w/ iteration\")\n",
    "    )\n",
    "    \n",
    "    # plot the data, the hypothesis, and true line\n",
    "    scatter_plots = get_hypothesis_scatter_plots(\n",
    "        pstv_pnts=pstv_data[:, 1:],\n",
    "        ngtv_pnts=ngtv_data[:, 1:],\n",
    "        hypothesis_params=params_history[-1],\n",
    "    )\n",
    "    for plot in scatter_plots:\n",
    "        fig.add_trace(plot, row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Feature 1 (-b/c)\", row=1, col=1, **common_axis)\n",
    "    fig.update_yaxes(title_text=\"Feature 2 (-a/c)\", row=1, col=1, **common_axis)\n",
    "    \n",
    "    \n",
    "    # the range of gradient/intercept values\n",
    "    grad_plot_range = np.linspace(-10, 10, 101)\n",
    "    intcpt_plot_range = np.linspace(-10, 10, 101)\n",
    "\n",
    "    # a + bx + cy = 0  ->  y = -b/c x - a/c\n",
    "    c = params_history[-1][2]\n",
    "    a_plot_range = -intcpt_plot_range * c\n",
    "    b_plot_range = -grad_plot_range * c\n",
    "    \n",
    "    # get all the [a, b, c] parameters for the contour map grid\n",
    "    grid_a, grid_b = np.meshgrid(a_plot_range, b_plot_range)\n",
    "    grid_a = np.reshape(grid_a, (-1))\n",
    "    grid_b = np.reshape(grid_b, (-1))\n",
    "    grid_params = np.stack([grid_a, grid_b, np.full_like(grid_a, c)], axis=-1)\n",
    "    \n",
    "    # compute the cost at each parameter set\n",
    "    grid_costs = compute_cost(\n",
    "        pstv_data=pstv_data, ngtv_data=ngtv_data, params=grid_params.T\n",
    "    )\n",
    "    grid_costs = np.reshape(grid_costs, (b_plot_range.size, a_plot_range.size))\n",
    "    \n",
    "    # plot the cost contour map against m and c parameters\n",
    "    fig.add_trace(\n",
    "        go.Contour(x=grad_plot_range, y=intcpt_plot_range, z=grid_costs.T),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # convert [a,b,c] parameter values into gradient / intercept\n",
    "    # a + bx + cy = 0  ->  y = -b/c x - a/c\n",
    "    x_history = -params_history[:,1] / params_history[:,2]\n",
    "    y_history = -params_history[:,0] / params_history[:,2]\n",
    "    # plot the history of gradient/intercept values on the cost contour map\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_history,\n",
    "            y=y_history,\n",
    "            mode=\"markers+lines\",\n",
    "            marker_size=[5]*(len(x_history)-1) + [10],\n",
    "            marker_color='white',\n",
    "            name=\"params history\"\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    # draw a marker for true gradient/intercept \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[decision_boundary_true_m],\n",
    "            y=[decision_boundary_true_c],\n",
    "            mode=\"markers\",\n",
    "            marker_size=10,\n",
    "            marker_color='yellow',\n",
    "            marker_symbol='x',\n",
    "            name=\"True params\"\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    # text annotation giving the current c value\n",
    "    fig.add_annotation(\n",
    "        text=f\"Current c = {c:.6f}\",\n",
    "        x=np.min(grad_plot_range),\n",
    "        y=np.max(intcpt_plot_range),\n",
    "        font=dict(color=\"black\", size=14),\n",
    "        bgcolor=\"white\",\n",
    "        showarrow=False,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\",\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"-b/c (gradient)\",\n",
    "        range=[np.min(grad_plot_range), np.max(grad_plot_range)],\n",
    "        row=1, col=2, **common_axis\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"-a/c (intercept)\",\n",
    "        range=[np.min(intcpt_plot_range), np.max(intcpt_plot_range)],\n",
    "        row=1, col=2, **common_axis\n",
    "    )\n",
    "    \n",
    "    # plot history of the cost against training iterations\n",
    "    cost_history = compute_cost(\n",
    "        pstv_data=pstv_data, ngtv_data=ngtv_data, params=params_history.T\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(1, len(cost_history) + 1)),\n",
    "            y=cost_history,\n",
    "            mode=\"markers+lines\",\n",
    "            name=\"cost\",\n",
    "        ),\n",
    "        row=2, col=1,\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Step\", row=2, col=1, **common_axis)\n",
    "    fig.update_yaxes(title_text=\"Cost\", row=2, col=1, **common_axis)\n",
    "    \n",
    "    # move the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.05,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Generate some (fake) data to do supervised classification\n",
    "####################################################################\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "N = 100\n",
    "data_spread = 2\n",
    "margin = 0\n",
    "\n",
    "decision_boundary_true_m = 2   # np.random.uniform(low=-5, high=5)\n",
    "decision_boundary_true_c = -3  # np.random.uniform(low=-5, high=5)\n",
    "\n",
    "pstv_pnts, ngtv_pnts = generate_fake_single_var_data(\n",
    "    n_points=N,\n",
    "    decision_boundary_true_m=decision_boundary_true_m,\n",
    "    decision_boundary_true_c=decision_boundary_true_c,\n",
    "    points_spread=data_spread,\n",
    "    margin=margin,\n",
    ")\n",
    "\n",
    "# add a column of 1s to make all the subsequent computation easier\n",
    "pstv_data = np.concatenate([np.ones((pstv_pnts.shape[0], 1)), pstv_pnts], axis=-1)\n",
    "ngtv_data = np.concatenate([np.ones((ngtv_pnts.shape[0], 1)), ngtv_pnts], axis=-1)\n",
    "\n",
    "####################################################################\n",
    "# Plot the generated data (and the underlying decision boundary)\n",
    "####################################################################\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=dict(\n",
    "        xaxis_title=\"Feature 1 (-b/c)\",\n",
    "        yaxis_title=\"Feature 2 (-a/c)\",\n",
    "        title=(\n",
    "            \"True decision boundary: \"\n",
    "            f\"gradient: {decision_boundary_true_m}, \"\n",
    "            f\"intercept: {decision_boundary_true_c}\"\n",
    "        ),\n",
    "        title_x=0.5,\n",
    "        width=630,\n",
    "        height=500,\n",
    "        xaxis_range=(-4, 4),\n",
    "        yaxis_range=(-4, 4),\n",
    "    )\n",
    ")\n",
    "\n",
    "scatter_plots = get_hypothesis_scatter_plots(\n",
    "    pstv_pnts=pstv_pnts,\n",
    "    ngtv_pnts=ngtv_pnts,\n",
    ")\n",
    "for plot in scatter_plots:\n",
    "    fig.add_trace(plot)\n",
    "\n",
    "fig.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(\n",
    "    pstv_data: np.ndarray,\n",
    "    ngtv_data: np.ndarray,\n",
    "    params: np.ndarray,\n",
    ") -> float | np.ndarray:\n",
    "    \"\"\"Compute the cost at specified parameter values.\n",
    "    \n",
    "    Args:\n",
    "        pstv_data: (N, C) array of positive data points.\n",
    "        pstv_data: (M, C) array of negative data points.\n",
    "        params: (C) vector of current parameters. OR (C, P) array which\n",
    "            is a set of P different parameters.\n",
    "    \n",
    "    Returns:\n",
    "        Cost scalar value, OR if `params` is a (C, P) array then it\n",
    "        would be a P sized vector of costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    ############################################\n",
    "    # IMPLEMENT THIS\n",
    "    ############################################\n",
    "    if params.ndim == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.full(fill_value=0, shape=params.shape[1])\n",
    "\n",
    "\n",
    "def gradient_params(\n",
    "    pstv_data: np.ndarray,\n",
    "    ngtv_data: np.ndarray,\n",
    "    params: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the gradient of the cost with respect to parameters\n",
    "    \n",
    "    Args:\n",
    "        pstv_data: (N, C) array of positive data points.\n",
    "        pstv_data: (M, C) array of negative data points.\n",
    "        params: (C) vector of current parameters.\n",
    "    \n",
    "    Returns:\n",
    "        Gradient of cost with respect to all different parameters. This\n",
    "        would be an vector of size (C).\n",
    "    \"\"\"\n",
    "    \n",
    "    ############################################\n",
    "    # IMPLEMENT THIS\n",
    "    ############################################\n",
    "    return np.zeros_like(params)\n",
    "\n",
    "    \n",
    "def gradient_descent_step(\n",
    "    pstv_data: np.ndarray,\n",
    "    ngtv_data: np.ndarray,\n",
    "    curr_params: np.ndarray,\n",
    "    learning_rate: float\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Runs a single step of gradient descent\n",
    "    \n",
    "    Args:\n",
    "        pstv_data: (N, C) array of positive data points.\n",
    "        pstv_data: (M, C) array of negative data points.\n",
    "        curr_params: (C) vector of current parameters.\n",
    "        learning_rate: Step size for gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "        (C) vector of updated parameters after gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    ############################################\n",
    "    # IMPLEMENT THIS\n",
    "    ############################################\n",
    "    return curr_params.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e8675",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Run gradient descent for n_steps\n",
    "####################################################################\n",
    "\n",
    "n_steps = 1\n",
    "\n",
    "# some initial hypothesis (can be any random value)\n",
    "hypothesis_params = np.array([5, 5, 1])\n",
    "\n",
    "# the step size used during gradient descent\n",
    "learning_rate = 1e0\n",
    "\n",
    "# used for plotting the progress of the parameters\n",
    "params_history = [hypothesis_params]\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    # compute the current cost (loss)\n",
    "    cost = compute_cost(\n",
    "        pstv_data=pstv_data,\n",
    "        ngtv_data=ngtv_data,\n",
    "        params=hypothesis_params,\n",
    "    )\n",
    "    \n",
    "    # get the new slope / intercept by gradient descent\n",
    "    hypothesis_params = gradient_descent_step(\n",
    "        pstv_data=pstv_data,\n",
    "        ngtv_data=ngtv_data,\n",
    "        curr_params=hypothesis_params,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # a + bx + cy = 0  ->  y = -b/c x - a/c\n",
    "    hypothesis_m = -hypothesis_params[1] / hypothesis_params[2]\n",
    "    hypothesis_c = -hypothesis_params[0] / hypothesis_params[2]\n",
    "    print(f\"Step {idx: <3},  Cost: {cost:.5f},  {hypothesis_params} = ({hypothesis_m}, {hypothesis_c})\")\n",
    "\n",
    "    params_history.append(hypothesis_params)\n",
    "\n",
    "# plot the whole gradient descent process\n",
    "fig = plot_gradient_descent_info(\n",
    "    pstv_data=pstv_data,\n",
    "    ngtv_data=ngtv_data,\n",
    "    params_history=np.stack(params_history, axis=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a332e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
